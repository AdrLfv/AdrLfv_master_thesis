\chapter{Sign Language Learning Game in AR}

\section{Introduction}

Sign Language is considered the main communication tool for deaf or hard of hearing people. It is a visual language that uses movements to provide people communication with the world. 

This sign language is too little used because it requires a significant investment to learn it and most people do not use it directly in their lives. SL is not understood by everyone, forming a communication gap between the mute and the able people. According to the World Health Organization (WHO) report, the number of people affected by hearing disability in 2020 was 466 million whose 34 million are children \cite{el2020sign}. Over 900 million people will have this disability by 2050.

In France in 2020, there were approximately 4 and 5 million deaf or hard of hearing people who have difficulties or are simply unable to communicate through speech. Concerning the deaf speakers of French Sign Language (LSF), the figures are uncertain: they oscillate between 80,000 and 120,000, depending on the sources.

In this work, we introduce a short video game to teach sign language. The application is intended to work on the DVIC Interactive Mirror but is fully functional on a simple PC.

This report deals with the implementation of an AI for sign language recognition (SLR) using the mediapipe framework in order to extract the user’s coordinates and to analyze them through a model in pytorch. The model is then implemented in a game engine to create a visual novel video game. The game is then adapted to an augmented mirror to allow the user to play it in augmented reality. The game stimulates the motivation of the player in order to favorise his learning.

\subsection{GOSAI for Augmented Mirroir}

\paragraph[short]{GOSAI}

GOSAI is a new framework to help the development of augmented interfaces on the computer with a display. This framework targets all developers, from beginners to experts. GOSAI offers
basic and often used augmented reality functionalities.

Its structure allows for components to be reused between
projects, thus building a general catalog of tools and
solutions that develops over time.

In addition, the framework uses mainstream programming languages to allow a wide range of developers to
use it. The framework is written in Python and Javascript.
Python is used for the framework’s core components,
while Javascript is used for display.

JavaScript is a flexible programming language. It is one of the core
technologies of web development, and everyone can use it on both the
front-end and the back-end.
It is a versatile and robust language for video game. The developers can use JavaScript to make games using a variety of platforms and tools. They can use both 2d and 3d libraries in combination with JavaScript to create fully-fledged games in the browser or external game engine platforms \cite{javascriptgaming}.

\paragraph[short]{The interactive mirror}

The following projects presented in this thesis are implemented on an augmented mirror running on the GOSAI software system.

The augmented mirror is a platform that provides extensive interaction between the real and the virtual world. The objective is to create a platform that can provide not only recreational use but also medical and educational use.

A one-way mirror is placed against a screen. The mirror reflects perfectly where the screen is black and can display  information when the pixels are emitting light. A camera is placed at the top of the mirror, facing slightly down.  A laptop is placed at the back of the screen (see figure \ref{fig:mirror_062}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{AdrLfv_master_thesis/images/mirror_062}
    \caption{The Augmented Mirror}
    \label{fig:mirror_062}
\end{figure}

% TODO resize l'image

The mirror can scan the room and detect and position objects the user interacts with. The augmented mirror uses the Intel D435 camera to estimate the position of a user in front of the mirror with Mediapipe. Thus, it can add a new dimension of interaction exploiting kinesthesia. This dimension is an advantage over traditional interfaces using touch or a mouse. 

This interface is ideal for the development of applications requiring movement. This is why it is relevant for the implementation of AI-assisted sign language learning modules.

\section{Related work}

\subsection{Sign Language Recognition}

A wide range of domains use SLR for different purposes. It can be found in robotics, human services, games, virtual reality applications, direct or remote communication or HCI projects \cite{adeyanju2021machine} (see figure \ref{fig:Polhemus}).

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/polhemus_tracker.jpg}
    \caption{Polhemus}
    \label{fig:Polhemus}
\end{marginfigure}

Many early SLR systems used data gloves and accelerometers to acquire specifics of the hands. The devices mesure x,y,z, orientation, velocity directly using a sensor such as the Polhemus tracker \cite{413199} \cite{5738842} or DataGlove \cite{Kadous1970} \cite{Metaxas1970} including accelerometers, gyroscopes, and electromyography sensors  (see figure \ref{fig:data_gloves}). Those techniques did not allow full natural movement and constricted the mobility of the signer, altering the signs performed and being restrictive because of the need of supplementary material.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/42979_2021_827_Fig3_HTML.png}
    \caption{Human–computer interaction using: a CyberGlove-II \cite{cyberglovesystems}, b vision-based system}
    \label{fig:data_gloves}
\end{marginfigure}

Most techniques based on data gloves convert the position of fingers and hands according to their angles into electrical signals to obtain the desired sign. 

In 2010, the ImageNet files appeared \cite{li2010crowdsourcing}. They provided a basis for the CNNs and deep learning models used today. This was the beginning of computer vision. In 2012, AlexNet appeared and dramatically reduced the error rate for image recognition \cite{alom2018history}. After the appearance of these models, the use of data gloves is gradually abandoned to focus on the implementation of modules using computer vision.

Computer vision based techniques use pose estimation on the face, body, hands and fingers to detect their position. This method uses images or videos of the signs through the use of a camera and calculations on the images assisted by artificial intelligence \cite{adeyanju2021machine}. 

The identification of signs must take into account many different parameters. Facial expressions and body posture are key in determining the meaning of sentences, e.g. eyebrow position can determine the question type. Some signs are distinguishable only by lip shape, as they share a common manual sign \cite{cooper2011sign}. 


\begin{marginfigure}
    \centering
    
    \includegraphics{AdrLfv_master_thesis/images/custom_gloves.png}
    \caption{Various custom gloves constructed by researchers in the sign language recognition field.}
    \label{fig:custom_gloves}
\end{marginfigure}

The speed of the sign realization can change the notion of speed induced by the performed sign. A sign can also depend on its position on the body. All limbs must therefore be taken into account during the analysis. These challenges include sensor placement, data collection and preprocessing, and model training and evaluation \cite{9178440} (see figure \ref{fig:custom_gloves}).

Sign language recognition systems based on computer vision and wearable sensors have been proposed by several researchers \cite{ionescu2005dynamic} \cite{yu2010vision} \cite{li2015feature} \cite{sonkusare2015review} \cite{bobic2016hand} \cite{islam2017real} \cite{islam2017real} \cite{saha2018machine}, \cite{rastgoo2021hand} \cite{xu2021application}. 

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/nimisha}
    \caption{Typical Vision Based Sign Language Recognition architecture.}
    \label{fig:nimisha}
\end{marginfigure}

Most recent SLR techniques use various image or vision based SLR systems comprising feature extraction and classification \cite{nimisha2020brief} (see figure \ref{fig:nimisha}). 

Many projects using computer vision assisted SLR exist \cite{admasu2010ethiopian} \cite{deriche2019intelligent} \cite{ahram2021advances} \cite{song2021intelligent} \cite{lee2021american} \cite{lee2021comparative} \cite{gao2021rnn}. 

A large part of these projects use a Convolutional Neural Network (CNN) model for predicting American Sign Language alphabet \cite{bin2019study}. Previously, different classifiers like support vector machine \cite{savur2015real}, random forest, multilayer perceptron, transfer learning, fine tuning \cite{saleh2020arabic} etc. have been introduced for sign language recognition on simple images. Recently, shallow CNN and Capsule Networks have obtained better results \cite{hasan2020classification}. 

Skeleton coordinate-based action recognition with coordinates has recently been attracting more and more attention to compute sign language videos because of its invariance to subject or background, whereas skeleton coordinate-based SLR only takes the data that is important for its learning. 

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/cao.png}
    \caption{Top: Multi-person pose estimation. Body parts belonging to the same person are linked, including foot keypoints (big toes, small toes, and heels). Bottom left: Part Affinity Fields (PAFs) corresponding to the limb connecting right elbow and wrist. The color encodes orientation. Bottom right: A 2D vector in each pixel of every PAF encodes the position and orientation of the limbs.}
    \label{fig:cao}
\end{marginfigure}

The most commonly used pose estimation frameworks that extract coordinates from a person using pose estimation are for example OpenPose \cite{cao2017realtime} (see figure \ref{fig:cao}), MoveNet \cite{movenet}, PoseNet \cite{kendall2015posenet} and MediaPipe \cite{lugaresi2019mediapipe}.



The recovered coordinates (extracted with computer vision or thanks to data gloves) are then processed with training methods. Various machine learning algorithms are then used for sign language recognition, including neural networks, support vector machines, and hidden Markov models \cite{9178440}.

Adeyanju et al. provides a comprehensive review of the state-of-the-art techniques used in sign language recognition using machine learning \cite{almeida2014feature} \ref{fig:S0957417414003042}. The paper highlights the significance of sign language recognition and its potential to revolutionize the way communication is done between the deaf community and the hearing community. The authors then review the different machine learning techniques used for sign language recognition, such as Hidden Markov Models (HMMs), Support Vector Machines (SVMs), and Deep Neural Networks (DNNs).
The authors also highlight the importance of datasets in sign language recognition and review some of the commonly used datasets for sign language recognition. They emphasize the need for large, diverse datasets to train machine learning models effectively.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/S0957417414003042.jpg}
    \caption{Feature extraction in Brazilian Sign Language Recognition based on phonological structure and using RGB-D sensors}
    \label{fig:S0957417414003042}
\end{marginfigure}

However, sign language is far more than just a collection of well specified gestures.

%TODO ajouter le concours de google

\subsection{Sign Language Learning Video Games}

The video game is a dynamic audiovisual entertainment platform, accessible and stimulating the imagination of players. There is a possibility of using them to strengthen skills and abilities within society. This is how video games are becoming a playful phenomenon, important in children's and youth culture.
Video games enhance the function of the attentional system, stimulate visual attention, reduce reaction time, improve the ability to discriminate shape and color, plus efficiency when following multiple object \cite{green2006enumeration}.

They are a good didactic way to promote interest and motivation by linking playfulness and pedagogical functions \cite{tejeiro2009efectos} .

The augmentation of interfaces thanks to technology causes a better attractiveness of the learning method and thus an increase of the time voluntarily dedicated to self-learning and of the motivation to concentrate on the method \cite{baker1994}.

Very few games use sign language as a basic element in the gameplay. We can cite Moss \cite{moss2018} (see figure \ref{fig:moss}), a video game on PlayStation VR in which the hero communicates with the player with ASL. 

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/moss.jpg}
    \caption{Moss hero communicating with the player through american sign language}
    \label{fig:moss}
\end{marginfigure}

Zahoor Zafrulla et al. present Copycat, a game designed to improve the American Sign Language (ASL) skills of deaf children \cite{zafrulla2011copycat}. The game was developed by a team of researchers from the University of California in collaboration with members of the deaf community.
The game, called CopyCat, is a digital game that uses machine learning to provide feedback to the players. The game consists of a series of mini-games that focus on different aspects of ASL, such as finger spelling, vocabulary, and grammar (see figure \ref{fig:aslgamecopycat}). In each mini-game, the player is presented with a video clip of a person signing a word or phrase in ASL. The player is then asked to copy the sign or phrase using their own signing.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/aslgamecopycat.jpeg}
    \caption{Screenshot of ASL Game Interface and the input devices for user  }
    \label{fig:aslgamecopycat}
\end{marginfigure}

CopyCat uses machine learning to analyze the player's signing and provide feedback on their performance. The game is designed to be adaptive, meaning that it adjusts the difficulty level based on the player's skill level. The game also tracks the player's progress and provides feedback on areas where they need improvement.
CopyCat developpers then enhanced their SLR system with the Kinect depth-mapping camera which uses colored gloves and embedded accelerometers to track children's hand movements \cite{zafrulla2011american}.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/bouzid.png}
    \caption{The interpretation of the sign “house” via tuniSigner}
    \label{fig:bouzid}
\end{marginfigure}

Bouzid et al. explore the use of a learning game for SignWriting, a system for writing sign languages, to enhance sign language learning for students \cite{bouzid2016using}. The game is designed to be played on a computer or tablet and includes a variety of activities such as matching signs to their written symbols and translating written symbols into signs. The authors used a 3D human character to interpret the SignWriting notation. An avatar based sistem called tuniSigner \cite{bouzid2013avatar} (see figure \ref{fig:bouzid}).

Lesmes et al. discusses the development of educational video games for deaf children in order to facilitate their inclusion in mainstream educational settings \cite{lesmes2022design}. 
The paper outlines the design and development process of educational video games, including the use of a participatory design approach that involved deaf children and educators throughout the design process. 
The games were designed to incorporate sign language, visual cues, and other features that would make them accessible to deaf children. The user can see an objective written on the left part of the interface, a character in the center, and an interpreter on the wright (see figure \ref{fig:lesmes}).

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/lesmes.png}
    \caption{Start interface of the videogame "Life of the Dinosaurs".}
    \label{fig:lesmes}
\end{marginfigure}

Most of the mobile applications dedicated to sign language learning are simple quiz with a lesson part and a questionnaire part. PocketSign is an application for learning American Sign Language through interactive activities (see figure \ref{fig:pocketsign}). 
The project offers learning lessons, a dictionary for translating words into sign language videos, as well as a "fingerspelling" mode in which the user watches video tutorials of words or letters of the alphabet and then has to repeat them. 

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/643x0w.jpg}
    \caption{Example of exercise with tutorial in Pocket Sign}
    \label{fig:pocketsign}
\end{marginfigure}

The application uses the phone's camera to film the user and verify that he or she is doing a word correctly before validating it and moving on to the next word.

\subsection{Visual Novel Engines}

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/nyuchan.png}
    \caption{Presentation of the visual novel engine P5VN}
    \label{fig:nyuchan}
\end{marginfigure}

P5VN is an open-source Interactive Design and Media Major at NYU Tandon School of Engineering student project, allowing to create a visual novel using P5.js. It allows to display dialogues, sprites, as well as backgrounds and use menus by clicking (see figure \ref{fig:nyuchan}). The game scenario is easily editable through a script, which is then parsed by the engine. The original aim of the project was to implement a prototype engine based in p5.js with a custom scripting language.  

Other visual novel engine, Tuesday JS \cite{TuesdayJS} or Monogatari \cite{monogatari} are simple web-based visual novel editor that can be used in a web browser. They are written in JavaScript allowing the use of vector graphics svg, gif animations and css styles.

Tuesay.js is an easy to use visual novel editor. Free and open source, it runs on any web browser. It is written in JavaScript, does not use third-party libraries and therefore does not require additional software installation. It uses a drag-and-drop interface for editing scenes and creating interfaces. The script is displayed as a flowchart with all the elements and branches of the plot. This makes it easy to navigate and helps you create stories with many plot options.

Monogatari.io is similar to Tuesday.js. The platform support different medias (images, videos, music), multiple languages. It is highly customizable, open source and multi-platform. 

JavaScript is a language adapted for the creation of projects as this one. 
The language is used extensively in games that do not require many resources. A 2D interactive game displays only a few details and elements.

Frameworks like Phaser JS library or p5.js are suitable for coding a simple game. p5.js is a JavaScript library for creative coding, focusing on making coding accessible and inclusive for artists, designers, educators, beginners, and anyone else. It can display graphics and process many different elements.

\subsection{Animated 3D Avatar}

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/saragih.png}
    \caption{Example of a real-time avatar animation from a single image thanks to semantic expression transfer.}
    \label{fig:saragih}
\end{marginfigure}

For the creation of an animated avatar, most systems implement the display of an animated model from an animation software (Blender, 3DS Max, Maya, Unity, Houdini...). The animations are worked directly in the software and then imported into a program for display in a game. 
Some projects allow the animation of a 3D model directly from the motion capture. In the project "Real-time Avatar Animation from a Single Image" \cite{saragih2011real} Saragih et Al. realize the modeling of 3D models from a simple photograph (see figure \ref{fig:saragih}). 

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/pose_animator.png}
    \caption{Animating full body character using FaceMesh and PoseNet with TensorFlow.js.}
    \label{fig:pose_animator}
\end{marginfigure}

A tool such as Unity or Maya can make its animation from MediaPipe coordinates. A tool that gives nice rendering for 2D animation is Pose Animator. A demonstration works with FaceMesh and PoseNet (from MediaPipe) online \cite{pose_animator} (see figure \ref{fig:pose_animator}). Unfortunately, this one does not understand the finger movements necessary for precise sign language tutorials.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/nguyen2021.png}
    \caption{Holistic tracking applied to a video frame. A) is the annotated original footage where
    the red dots are the tracked landmarks, while the green lines connect the joints. B) is the same
    frame in Blender with landmarks plotted as orange spheres and cyan coloured bones. C) shows
    the motion capture data applied to an avatar. \cite{nguyen2021automatic}}
    \label{fig:nguyen2021}
\end{marginfigure}

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/blazepose.png}
    \caption{BlazePose results on yoga use-cases}
    \label{fig:blazepose}
\end{marginfigure}

Another way is to adapt the coordinates in real-time to animate a character on Blender using Daz Studio as Nguyen et Al. \cite{nguyen2021automatic} (see \ref{fig:nguyen2021}). Most interactive avatars with pose estimation use MediaPipe and TensorFlow.js (namely FaceMesh, BlazePose and HandPose) \cite{blazepose} (see \ref{fig:blazepose}).

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/posenet_demo.jpg}
    \caption{React project that will allow us to move a 3D model with Three.js (React Three Fiber) and Tensorflow's Pose Estimation model (PoseNet).}
    \label{fig:posenet_demo}
\end{marginfigure}

Posenet\_demo \cite{posenet_demo} is a project allowing moving a 3D model with Three.js and Tensorflow’s Pose Estimation model (PoseNet). The model uses Tensorflow and PoseNet to detect the key points of the joints in each frame and then send those points over to the Model file. 

The project uses an Adobe Mixamo \cite{mixamo} 3D model in FBX format and Blender to import the FBX format and export a GLB format \cite{posenet}. The avatar follows the head and shoulder's inclination (see \ref{fig:posenet_demo}).

A last interesting example is that of Kalidokit \cite{kalidokit}. Kalidokit is a tool that uses Mediapipe\/Tensorflow.js models for tracking face, eyes, pose, and hand movements.
It is compatible with various models such as Facemesh, Blazepose, Handpose, and Holistic. The tool calculates simple euler rotations and blendshape face values based on the predicted 3D landmarks.

\begin{marginfigure}
    \centering
    \includegraphics{AdrLfv_master_thesis/images/kalidoface.jpg}
    \caption{KalidoKit can move 3D avatars by tracking the movement of face and body with a simple webcam.}
    \label{fig:kalidoface}
\end{marginfigure}

Kalidokit is the core component for Vtuber web apps, such as Kalidoface and Kalidoface 3D. Its purpose is to rig 3D VRM models and Live2D avatars. The project is a JS library intended for developers using Mediapipe pretrained models and not a complete app on its own. The library still has to be adapted to run on different platforms. The project is based on Three.js, a powerful library for creating three-dimensional models and games. 
With few lines of JavaScript, Kalidokit allows the creation of simple 3D patterns to photorealistic, real-time scenes. The library can create complex 3D geometrics, and animate and move objects.

Three.js is a JavaScript library for creating 3D scenes in a web browser. It can be used with the HTML5 canvas tag without the need for a plugin. The library enables the application of textures and materials. It also provides various light sources to illuminate scenes, advanced postprocessing effects, custom shaders, load objects from 3D modeling software... It is easy to use, intuitive, and a well-documented library.

\section{Gameplay}

The sign language game is an augmented reality game, a visual novel on the augmented mirror. We follow a character during a short adventure in which the user can take choices by making American Sign Language signs in front of the mirror. He can answer to the characters, interact with objects, choose actions and choose a path. 
Sometimes the player sees the tutorial of one, two, or three signs at the same time. The user must then make the sign of the choice he takes. For example he can choose to turn right or left by making the appropriate sign (see \ref{fig:sleg_left_right}). 

\begin{figure}[h]
    \centering
    \includegraphics[]{AdrLfv_master_thesis/images/sleg_left_right.png}
    \caption{Choice to go left or right in the sign language learning game.}
    \label{fig:sleg_left_right}
\end{figure}

3D avatars are the models for all the characters. Their limbs (including fingers) are animated. Only the character Aria (the hero's friend) is performing the sign tutorials.
The signs made are detected by the camera and guessed by an AI working in back end. Dialogue lines appear during the adventure to guide and discuss with the player. The user must make the "ok" sign with his hand to scroll the text.


\section{General Architecture}

\subsection{The Sign Language Video Game in Gosai}

The application is connected to the get\_sign and slr drivers of gosai to estimate the sign of a person in front of the mirror. The data is then transmitted to the engine.js which is in charge of running the game taking into account the user's movements (see \ref{fig:sleg_architecture}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.6\columnwidth]{AdrLfv_master_thesis/images/sleg_architecture.png}
    \caption{The Sign Language Video Game architecture in Gosai.}
    \label{fig:sleg_architecture}
\end{figure}

The engine.js accesses the script containing the whole game, and the components contained locally (sprites, animations or backgrounds).

\subsection{Visual Novel Engine}

\subsubsection{Engine implementation}

The best solution to make such a game was the implementation of a visual novel engine on the mirror from which to develop everything else in the game. P5VN engine is precisely  basically adaptable for the mirror because the engine is basically working with p5.js.

Previouly, p5VN was able to load and display background, sprites of characters, some text interactive with the mouse click, menus with multiple buttons for the user to take choices.
The engine was running synchronously in a single thread.
After an important adaption. The module is now running asynchronously to be compatible with gosai. 
The engine now loads and plays video animations automaticaly at launch, the user can now interact and select menus thanks to sign language and added many other features.

\subsubsection{Menu}

In a visual novel, the player does not interact directly with the keyboard but must click on menu buttons to make choices. As the user interacts with the mirror only by the estimated pose, the menu system had to be adapted to both enable debugging by clicking and making choices with movements.

Each time a menu appears, it takes the form of two or three words aligned and distributed horizontally on the screen. An avatar of Aria appears behind each word to demonstrate the sign related to this word (see \ref{fig:sleg_left_right}). The location of the words and the 3D tutorials are spread over the width of the screen according to the number of buttons in the displayed menu. 

Aria's avatar animation plays in a loop until the player makes a sign detected by the SLR module and included in the menu. The path taken by the game is then directed towards the continuity of the chosen branch.

\subsubsection{Script}

The game script is contained in a script.txt file in the application components. It contains a set of commands starting with the \$ symbol telling the game to do a particular action. These commands can be :
\begin{itemize}
    \item \$tag which indicates a specific point in the script
    \item \$jump to jump to a tag
    \item \$defineC to define a character (name, sprite address, text color)
    \item \$defineImg to define an image
    \item \$bg to display a background
    \item \$show to display a character
    \item \$addAnimation to play an animation to a character
    \item \$setSprite to display the sprite of a character
    \item \$menu to create a new menu
    \item \$hide to hide a character
    \item \$setvar to set a variable to a value
    \item \$if to manage if then else commands
\end{itemize}

All the loading of sprites, animations, their playback, display are now managed automatically at startup in asynchronous instead of being set manually in the script or in the code as before. The engine running in synchronous mode at the beginning, a sprite could not be called before being loaded. The game starts on a loading time at startup corresponding to the loading of sprites and animation videos in the memory of gosai. This allows each animation to be played instantly when each one is called.

\subsection{Animations}

The most efficient way to create accurate custom 3D animations for the animated sign language tutorials was to implement a module allowing to control an avatar in 3D through pose estimation and then record the movements produced or a video of the animation.

An application to control an avatar remotely thanks to the estimation pose has been developed on the augmented mirror. The avatar is able to track and copy accurately the position of the user's limbs. The created application is now in free demonstration on the mirror. It also allowed to create all the 3D character animations on the sign language learning game.

\subsubsection{Esla}

The Educative Sign Language Avatar (ESLA) is a 3-dimensional avatar controllable in a gosai application following the estimated MediaPipe pose. 

\begin{figure}[h]
    \centering
    \includegraphics[width=2.8\columnwidth]{AdrLfv_master_thesis/images/esla_architecture.jpg}
    \caption{ESLA architecture on GOSAI.}
    \label{fig:esla_architecture}
\end{figure}

The program uses the driver of the mirror camera, processed by an estimation module (see \ref{fig:esla_architecture}). The data is then transmitted to the back end of the application in python through the Redis database. The calculations related to three.js and the display is finally managed in the front end written in Javascript. The data is transmitted from the back end through a socket.io server.
Its movements copy live those of the user in front of the mirror \cite{esla}. It was the first version of the interactive avatar on the augmented mirror.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.4\columnwidth]{AdrLfv_master_thesis/images/esla.png}
    \caption{The Educative Sign Language Avatar (ESLA).}
    \label{fig:esla}
\end{figure}


An animated 3D character appears on top of the scenery at the launch of the application (see \ref{fig:esla}). Its animation is made possible by playing features using MediaPipe and OpenCV. The program can retrieve the coordinates of a user’s position in real time. These coordinates are processed, and the avatar copies the movements Animation Retargeting, a video game animation technique using motion capture.

The programs retrieves the links between each point of the pose provided by MediaPipe and Three.js calculates and display the 3D part. 

Three.js entirely manage the loading of the character, its display, its animation, the rendering, the light and the camera .

Three.js uses a pivot system with matrix4 and quaternions. However, some functions to transfer rotations from one base to
another are missing (even though they are very much in demand by the community). Therefore, the entire limb rotation system was implemented by hand.
MediaPipe first retrieves the (real) coordinates of the user. These coordinates are converted into the world base (x-axis to the right, y-axis to the top, z-axis to the camera) of the three.js scene with transformations. The program retrieves the coordinates of the top of the spine, the shoulder, and the elbow. It creates two vectors spine/shoulder and shoulder/elbow. The coordinates of these two vectors are then read from the base of the parent of the bone to be rotated (here the left shoulder). An algorithm then calculates a quaternion containing the rotation data from the first vector to the second.

This quaternion is then applied to the limb, which takes in its local base the same rotation as the user’s arm with his shoulder.
This allows you to make precise rotation transfers from one base to another and thus animate all parts of the avatar.

Unfortunately, the project lacks a lot of precision in the movements, especially at the level of the fingers, which is very inconvenient for the creation of accurate tutorials in sign language. Another more efficient solution for the creation of 3D animations with the estimated pose had to be implemented.

\subsubsection{Kalidokit module implementation}

The creation of an avatar controllable with Mediapipe on the mirror has been by the implementation of the Kalidokit solver. This JS library allowing the animation of arms, hands, fingers, face, mouth, eyelids, pupil of VRM models (Virtual Reality Model). The VRM is formulated on the basis of the standard 3D format glTF2.0 to manage humanoid models. It aims to be particularly expressive. This type of model has a large number of articulations, can blink and animate its mouth. It is very often used in the world of VR games (VR chat for example), or by Vtubers (entertainment broadcasters who use a virtual avatar).  

The models used in the game were downloaded from the site Vroid Hub cite\cite{vroid}. They must respect specific conditions of use: use by a third party, downloadable, use as an avatar, commercial use by a company. The avatar must also be sober. The one that has been selected to be implemented in the demo application allowing to control the avatar in augmented reality on the mirror is called "papa\_de\_him\_chan". 

The model is easily changeable locally in the application created to allow the user to manipulate different characters. It is with this application that will be created the animations of tutorials of sign language for the learning game on the mirror.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.6\columnwidth]{AdrLfv_master_thesis/images/aria_v1.png}
    \caption{The first version of the Augmented Reality Interactive Avatar (ARIA).}
    \label{fig:aria_v1}
\end{figure}

The implementation of Kalidokit on the mirror has been reworked to suit the operation of gosai, and the joints set to a greater degree of freedom to allow more flexible animations (see \ref{fig:aria_v1}).
Such an application therefore allows to record qualitative signs and poses for the characters and tutorials of the game.

The models used for the characters of Aria, the grandmother and the salesman were recovered on Vroid Hub and are free to use. The VRM models are mainly adapted for Vtubers, that's why the characters can look childish and can look like Japanese anime characters.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.4\columnwidth]{AdrLfv_master_thesis/images/iloveyou.png}
    \caption{Aria's character performing the "I love you" sign.}
    \label{fig:iloveyou}
\end{figure}

All sprites and 3D video animations of the avatars were recorded from the "Aria" application on the mirror. This includes 22 video animations and 14 sprites for the character of Aria in the sign language game (see \ref{fig:iloveyou}).

\section{Sign Language AI}

\subsection{Overview}

Sign language involves the usage of the upper part of the body, such as hand gestures, facial expression, lip-reading, head nodding and body postures to disseminate information \cite{adeyanju2021machine}.

The final goal of this work is to implement an SLR AI inside the sign language learning game. This AI allows the recognition of choices in the game thanks to the estimation of the user's pose but also the control of the game using commands linked to signs.

Many models with sign language recognition already exist. The work done here proposes a model, as well as an easy system of creation, dataset management, training and visualization of the data.

\subsection{Integration in Gosai}

The project of creating the SLR AI is apart from gosai \cite{slr\_mirror}. Only the weights are integrated in gosai in a module making the comparison between a sign made in front of the camera and the values of the weights recorded locally.

The video game retrieves the coordinates of the user and place them in tensors containing the data of a video of 30 frames.

The model is called by passing the data to it and a table of probabilities concerning each action is retrieved as a result. 

The guessed sign is then transmitted from the slr module through gosai’s redis database to the game engine. When the probability of a detected sign overfits a threshold, the sign is considered as validated by the game engine.

\subsection{Structure}

The project is divided into 6 different local modules and a main file that initializes the parameters and launches the different processes. The processes are an optional phase of dataset creation, a tutorial phase that displays the skeleton for data vizualisation, a preprocess phase that retrieves the data from the dataset and formats them, a data augmentation module, and a weight calculation and exportation in different file types.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.4\columnwidth]{AdrLfv_master_thesis/images/SLR_structure.png}
    \caption{}
    \label{fig:SLR_structure}
\end{figure}

The developed AI is a full-fledged project that does not require gosai to run. It takes place in 5 steps, each one being optional (see \ref{fig:SLR_structure}). These steps are: the creation of a dataset, the training, test and validation (with data augmentation or not), the conversion of the created files, the visualization of the recorded signs, as well as the visualization of the training and test results.

\paragraph{Dataset}

If a movement is included in the known actions but is not found in the files, the creation of the dataset of this movement is launched automatically. 

The dataset is created by the user and stored in a folder "MP data" created, and separated in three
folders : train, valid, test containing the actions (name of the movement). The recorded sequences are automatically separated between the three folders train (80\% of them), test (10\%) and valid (10\%). 

Each sequence is 30 frames each by default, including coordinates, that is to say and now 116 data per frame (that is to say 58 points) after having removed  431 points of the face and just Keeping 4. The 4 kept points are one for the chin, for the forehead, for the wright and left cheek.
It enables signs using the head to be better detected.

The coordinates are retrieved from Mediapipe during the creation of the dataset. The recording is done from the webcam of the device used, an intel real sense or the default pc webcam if the intel is not detected. When implementing on another device such as the interactive mirror, this driver must be modified to match that of the camera used.

At the start of the dataset creation, the user alternates one second of video recording, then two seconds to reposition in a loop for each sign. He has a video feedback on himself to check if he is well positioned in the plan.

\paragraph{The recorded signs visualisation}

The recorded signs visualization module (data\_vis) retrieves the coordinates of a video by action, stored in the dataset.
It sorts the coordinates of the points of each part of the body, finds the different links between each point and the posters, frame by frame. Arrays of all the sorted coordinates for each body part (Face if activated, Body, Hand) are used, as well as all the links of the body arrays.

For each point detected in the dataset, the visualization displays the links it has with the points it is linked to in the table. The program also display the action to which the visualization is associated, above the drawn character.

\paragraph{Preprocess}

The preprocess phase retrieves all the data from the dataset and places them in tensors to provide them to the model.
After creating the preprocess instances (according to their type: train, test, valid) we pass them to the dataloader. 

These pass an index to them (concerning a sequence among all the sequences attributed to the type of the preprocess). Their program calculates which action is concerned by the desired sequence and recovers all the coordinates of the frames of this sequence. 
In some papers, it is said that some sign language movements are only possible by giving some indications with lips or facial expressions \cite{cooper2011sign}. Some of the lip coordinates should therefore be kept in the dataset retrieval during the preprocess. For other papers, it is just not necessary to keep these points which take a lot of storage for nothing \cite{dreuw2007speech}.

The preprocess does not use all the coordinates of the face, this allows to have much faster train loops because all these points are not necessary.

\paragraph{Data augmentation}

The data augmentation is called during the pre-processing phase, just after retrieving the data of the concerned sequence. It is given the data as a parameter, it reviews the data and will apply a random horizontal and vertical shift and scale to it. This process artificially increases the number of positions in which the user can stand.

\paragraph{Model}

The model is composed of a bidirectional LTSM, a linear, a dropout, a batchnorm1d, a relu, and an other Linear. The AI trained on 16 different signs or 1600 sequences. The model reaches an accuracy of 87\% on the test set and 96\% on the training (see \ref{fig:slr_results}).

\begin{figure}[h]
    \centering
    \includegraphics[width=2.2\columnwidth]{AdrLfv_master_thesis/images/slr_results.png}
    \caption{Visualization of testing and training accuracy over 200 epochs. In yellow the accuracy without data augmentation, and in green with data augmentation.}
    \label{fig:slr_results}
\end{figure}

Recurrent Neural Networks (RNN) allow to process temporal sequences (language, videos, numerical data). They keep the memory of past data to predict data sequences in the near future. 
This is why LSTM (Long-Short-Term-Memory) and Bidirectional layers are used into the model. 
The LSTMs are composed of several gates (respectively 3 and 2) which allow to forget or to selectively memorize the information of the previous temporal sequence in a dynamic memory.
LSTMs have an internal memory that is constantly changing dynamically according to the input data sequences.

The training uses the optimizer AdamW, a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights \cite{loshchilov2017decoupled}. 

Models are automatically exported in pth format, a common PyTorch convention is to save models. They can also be exported to onnx (Open Neural Network Exchange) \cite{onnx}, an open format designed to represent machine learning models.

This extension enables greater interoperability in the AI tool community, allowing a large number of AI frameworks to be more compatible by allowing them to share models. This interoperability allows trained models to be easily deployed in different software platforms like GOSAI.

The onnx models retrieved from the project output are then directly implemented in the SLR module of gosai.


\subsection{Configuration}

All the processes mentioned above are optional. This management is located in a config.json file in which the user can enable or disable a large number of project features. 

All the options are : "actions" (all the signs recognized by the AI, "adapt\_for\_mirror" (to transform the coordinates specificaly for the augmented mirror),"convert\_files" (to convert the files from pth to onnx), "erase\_dataset" (to erase the dataset before recording a new one, by default the program automatically completes the existing dataset), "erase\_runs" (erase the local tensorboard files), "make\_data\_augmentation", "make\_dataset", "make\_train", "make\_tuto"  (visualization of the recorded movements), "nb\_epochs", "nb\_sequences" (total video number the user wants to record for a sign), "sequence\_length" (one video frame number).

The default actions are "nothing", "empty", "ok", "yes", "no", "left", "right", "house", "store", "hello", "goodbye", "television", "leave", "eat", "apple", and "peach". Those are all the signs used inside the SL video game.

The project aims for making it as easy as possible for the user to create weights according to his desired characteristics.

\subsection{Visualization of the results}

TensorBoard is a tool providing visualization solutions to machine learning tests. It allows to track and visualize metrics such as loss and correctness. TensorBoard allows displaying the graph of the exported model, displaying histograms of weights, biases or other Tensors, and many other functions.

Just by entering the "make set-up\_tensorboard" function in the terminal (or by doing a "make first\_boot"), the user launches the creation of a local server running on port 6006, retrieving the weights as the model is trained. 

A browser page then opens automatically to access the local server address. On this page the user can study a graph of the training and test phase (percentage of success according to the number of epochs). 

The logs of these results are stored locally, they are accessible and can be easily compared between them. This makes experimentation easier and more intuitive.

\subsection{Limitations}

After many experiments, several biases have appeared that directly affect the AI results. For example, the size of a user differing too much from the person who created the dataset is sometimes problematic, resulting in a strong difference in recognition performance between users with the same or different morphology. 

Since the AI bases its training on the data obtained from Mediapipe, the results of the SLR also depend on its parameters. It was observed that Mediapipe was better at recognizing the limbs of a person with a strong contrast with the background. 

This resulted in a very significant difference of about 15\% in performance on the training and test set. When the dataset was created by a person wearing a black shirt, his white skin was better detected by Mediapipe and better recognized afterwards. 

The same thing happened when using the sign recognition directly, with a t-shirt allowing a better contrast, the recognition was only favored. This also causes the fact that signs made on the belly (being a plain background without detail) are much better recognized than those made on other places with more potential details in the background.

\section{Usage scenario}

The augmented mirror is primarily aimed at families and is designed to be placed in a living room. Its use is very wide and allows to have fun as well as to learn by interacting.
The mirror does not require any additional material, so its use remains very simple.
The learning game of the sign language is directly launched from the menu of the mirror. The user just has to follow the instructions given by Aria to follow the course of the adventure. The game is primarily aimed at young children. Through repetition and distraction, they intuitively learn certain gestures.

\section{User study}

\subsection{Set Up}

As the sign language learning game is intended for young users, this test focuses on a sample of 12 people between 18 and 25 years old.
6 people watch a number of video sign language tutorials on the "pocket sign" application. The signs viewed correspond to those encountered in the game when choosing the path through the house ("ok", "yes", "left", "right", "house", "store", "hello", "goodbye", "television", "leave", "eat") or the path through the store ("ok", "yes", "no", "left", "right", "house", "store", "goodbye", "apple", "peach"). 4 people view the signs of the passage through the house and 2 view the signs of the passage through the store.
When they think they have understood the sign viewed, they skip to the next sign. 

6 other people use the sign language learning game on the mirror. They are told the principle of the game before using it. Two of them go through the house and four of them go through the store.

Both groups are told at the beginning that they will be evaluated on certain signs at the end of the study. 
For both groups, the session is timed from start to finish.

\subsection{Results}

\paragraph{Sign Language Video Game}

In one session, users took an average of 4.44 seconds from game launch to completion. Some users have experienced problems related to poor AI recognition.
Notably for the sign "peach" and the sign "eat" difficult for two users.

One user took 8:25 to complete the game. He had great difficulty in making the "go" sign in the house. This bias is explained by the fact that the dataset was created by a tall person (1,84m) and this user had a much smaller height (1,60m). 

Users were tested after their session on some random signs. Their average score was 2.33 signs learned out of 4. The errors made are related to several elements. The AI sometimes finds a sign far too quickly when the user has not yet placed his hands correctly. 

The user then has no time to look at the signs and make his choice. Another error is related to the fact that when there are three tutorials at the same time (menus with three choices), the user has more difficulty to concentrate on each one and his memorization of the signs is less efficient.

To correct the problem related to the too high detection speed of the AI, increasing the detection threshold value would be an efficient method.

To conclude, people were asked to note on 10 their opinion about diverse questions. Their average motivation to learn sign language in this particular way (with an augmented mirror and the video game) was 7,91/10.
The average fun in interacting notation was 8,25/10. Their ease of understanding what needs to be done had a notation 7,58/10. The practicality of using this learning method (augmented mirror) had a notation of 8,25/10.The effectivity of using this learning method received a notation of 6,41/10. And the capacity of the AI to guess the correct sign was noted 6,11/10.

The global feedbacks were that the avatar sometimes moves his fingers too much and could not be very precise. The fact that the display is in 2D (on the mirror screen) and not in 3D makes it difficult to understand the signs.
One user was blocked by the fact that the characters look like anime characters and not more realistic ones, which can put off the desire to practice.

Some users forgot the "ok" sign to skip dialogues during the game although the sign was shown twice at the beginning of the game. This prevented them from continuing the practice without external intervention to remind them of the sign. 
One user had learning difficulties due to the avatar's movements being considered unclear. Interacting with a real person, however, allowed him to learn more effectively. The lack of accuracy of the AI was also strongly criticized (rated 6.11/10 by users). This accuracy is almost perfect for the creator of the dataset, but low to other users with a different morphology.

The 3D animated tutorials were generally liked and were considered more attractive than simple video tutorials. The fact that the game was implemented on an augmented mirror was praised because this interface is considered to be mainstream and easy to integrate in a living room.


\paragraph{Videos}

\textit{POTENTIAL USER STUDY ABOUT LEARNING WITH VIDEO TUTORIALS}

\section{Discussions}

The SL Game stimulates the visual, verbal and kinesthetic intelligence. 

The visual intelligence is the ability to create mental images, and to perceive the visible world with precision in its three dimensions. Viewing 3D sign language tutorials and having to "decode" the different movements and positions of the avatars' limbs stimulate the user's visual intelligence. The user needs to perceive, to make a mental projection before re-performing the identified signs.

Verbal intelligence is the ability to read, speak easily and connect mental ideas into words. Having to connect a picture to a word in the project leads to the ability to express oneself fluently and therefore to connect the mental idea of a word into a gesture. 

Kinesthetic intelligence is the ability to use one's body in a fine and elaborate way, to express oneself through movement, to control one's body movements well. This type of intelligence is stimulated through play in this project because the brain must translate the mental image of body positions and movements in space into real positions and movements. 

The stimulation of three different intelligences, added to the motivating gamification aspect, favors information retention. The user can remember a particular gesture by the memory of having seen it and projected it mentally, by the memory of having given it a linguistic meaning, and by the memory of having performed it physically in space.

\section{Conclusion}

\subsection{Playful Learning and ASL}

Playful learning and gamification are learning methods that use playful elements to facilitate the assimilation of knowledge. 

Learning video games are a popular form of these approaches and are often used to teach practical skills such as sign language. The use of augmented reality, pose estimation, and artificial intelligence in the Sign Language Video Game creates an immersive and interactive experience for the learner to practice reproducing signs correctly in real time. 
This approach promotes learning by providing instant feedback and allowing the learner to practice as many times as they want. 

In addition, the use of cutting-edge technology often generates interest and engagement in the learner, which can help to increase motivation and perseverance in learning sign language.

\subsection{Limitations}

The results of the user studies make it possible to extract a certain number of limitations from the project.

The size difference between the person who created the dataset and a potentially taller or shorter user is problematic. This can be solved by applying a compression of the coordinates to the horizontal and vertical axis randomly in the data augmentation or by registering the dataset by people of different sizes.

The dataset produced and used is small. It contains little noise, little variation, and little mixing. The model is also moderately efficient and could be reworked especially with the use of transformers. The model is also trained on few different signs (15). This means that in the case of an enlargement of the dataset, the accuracy would tend to decrease even more. It is therefore necessary that the model and the dataset be reworked in the future.

Another important limitation is the sometimes difficult clarity of avatars in sign language tutorials. The pivot angles of each of the avatar's bones being limited to avoid glitches and ragdolls, it is sometimes hard to record precise signs especially at the finger level. The user must then imitate and learn a model that is not totally accurate. The avatar can be slightly reworked to widen the achievable angles and thus gain in precision.

\subsection{Future Works}

The current system is good for direct practice but too easy because users can afford to just copy the sign. It would be necessary to add an evaluation dimension with signs that come up in the potential choices without giving the tutorials every time. Thus the user will have to force himself to remember and will have a better learning by repetition.

This work has already been done in another application on the mirror similar to a sign language training. The application consists of successive viewings of sign language tutorial videos, practice, and finally correction. 

The correction phase consists of a frozen Mediapipe skeleton appearing on the mirror and performing the beginning of a sign (first frame of a recorded movement). 

The user also has a Mediapipe skeleton displayed on the mirror and following his position. When he superimposes his own skeleton on that of the model, the model's skeleton moves slightly to display the next frame. 

The user must then superimpose his skeleton on the second frame and so on until he has validated the 30 frames that make up a sign. The person must superimpose some points of his body, but especially his hands and in particular his fingers. This allows him to learn with precision the sign he has just made. The training then moves to a tutorial of another sign. 

The implementation of the correction phase of this training in the sign language game would have a real interest since it would allow the user to better remember the same sign by having to perform it several times, by focusing on the position of each finger. It also allows him to visualize the correct position he has to take in the space. 
This implementation would thus answer most of the current problems of the game. It had not been done because it broke the rhythm of the game during its progress, but its implementation at the end of the game as a reminder of what has been seen would be an effective method for learning.

%TODO rajouter des effets stylés en latex